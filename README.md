CREDIBILITY SCORE EVALUATION

Deliverable 1 https://github.com/anchalrai101/CREDIBILITY-SCORE/blob/main/deliverable1(propercommit).ipynb checks the credibility scores of online sources. The script fetches web content, checks for misinformation, and assigns a function score using machine learning. It integrates Hugging Face models and Google Fact Check API to improve accuracy. The final score helps users assess the reliability of information before trusting a source.

In deliverable 2 https://github.com/anchalrai101/CREDIBILITY-SCORE/blob/main/deliverable2(propercommit).ipynb we are using URL Credibility Validator, it is used to evaluate the credibility of web pages based on various factors. It also includes a function to generate a CSV file containing user prompts, URLs, and their credibility scores. When you run this function, it creates a CSV file named credibility_scores.csv and automatically downloads it to your device. You can see the csv here urltesting_scores.csv.

In deliverable 3 https://github.com/anchalrai101/CREDIBILITY-SCORE/blob/main/deliverable3(commit).ipynb we extracted data from github which are checking the credibility scores.  The model is trained using TensorFlow and can be easily deployed or downloaded from Hugging Face. We have prepared the data taking the extracted data and the code proceses the text data using a tokenizer, and trains the model while monitoring its accuracy. The validation loss is overfitting so it performs well on training data but poorly on unseen data. 

 I have deployed the model on hugging face aswell which can be accessed here https://huggingface.co/anchalrai101.
 



